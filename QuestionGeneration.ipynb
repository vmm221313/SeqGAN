{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dc4659ec9714836990d2a83f9a7f4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_999be904dbdb44e196af175037659b89",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_704c09ecfd4f47d38c0fc7861ed1f8fb",
              "IPY_MODEL_d1df2bce2ff54578b60015f96c7e91de"
            ]
          }
        },
        "999be904dbdb44e196af175037659b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "704c09ecfd4f47d38c0fc7861ed1f8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a3f3ce8887a44869eee7f7ae0638ea7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 159,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_32475c4a4be2414aa2929fe61147d189"
          }
        },
        "d1df2bce2ff54578b60015f96c7e91de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36db582cfc46484aa4112a64954d56c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 80% 159/200 [5:42:26&lt;1:28:11, 129.07s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_903fd2501e11479cb6157d5a185b063e"
          }
        },
        "4a3f3ce8887a44869eee7f7ae0638ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "32475c4a4be2414aa2929fe61147d189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36db582cfc46484aa4112a64954d56c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "903fd2501e11479cb6157d5a185b063e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgnnGwDUe5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f8b70c2e-2a31-4c5a-fe1d-c9c5f2a5f3c3"
      },
      "source": [
        "!pip install bcolz"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bcolz in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uCorZI-94vm",
        "colab_type": "code",
        "outputId": "e41bb95e-6d02-4165-a41d-4f7adffcd50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFv-hZRX96cE",
        "colab_type": "code",
        "outputId": "b67ce206-ca0d-47dd-fd83-b49a8f7c2ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd drive/My\\ Drive/Projects/QuestionGeneration"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Projects/QuestionGeneration\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fsxF5gI7Bs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import copy\n",
        "import json\n",
        "import bcolz\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as po\n",
        "#from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z127-Pig7BtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHnOVRp3ESEk",
        "colab_type": "code",
        "outputId": "f9b315f7-efd0-4036-be6c-73e9f03fddbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "#from transformers import BertModel\n",
        "#from transformers import BertTokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mlY1g3ORzCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.set_default_tensor_type(torch.cuda.FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ-m4BdB7BtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from generator import Generator\n",
        "#from discriminator import Discriminator\n",
        "#from target_lstm import TargetLSTM\n",
        "#from rollout import Rollout\n",
        "#from data_iter  import GenDataIter, DisDataIter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFbMYjlwP1IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = bcolz.open(f'conceptnet_embeddings/300.data')[:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP5e_kgiQLXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conceptnet = {w: vectors[i] for i, w in enumerate(words)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tszZwlH47BtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic Training Paramters\n",
        "SEED = 88\n",
        "BATCH_SIZE = 64\n",
        "TOTAL_BATCH = 200\n",
        "GENERATED_NUM = 10000\n",
        "POSITIVE_FILE = 'real.data'\n",
        "NEGATIVE_FILE = 'generated_questions.data'\n",
        "QUESTION_FILE = 'questions_real.data'\n",
        "EVAL_FILE = 'eval.data'\n",
        "PRE_EPOCH_NUM = 120"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-JJBZaA7BtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKmftjzf7BtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda. is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG3JAHLGCl_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = po.read_csv('data/train_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-wy4DqhpYvfv",
        "colab": {}
      },
      "source": [
        "pickle.dump(words, open(f'conceptnet_embeddings/300_vocab.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open(f'conceptnet_embeddings/300_lookup_table.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V59vJ5s5FLxP",
        "colab_type": "code",
        "outputId": "316633a4-b7de-4af2-b601-90c002b4a46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You fellas above says its the best Taiwanese r...</td>\n",
              "      <td>how many taiwanese restaurants are there in th...</td>\n",
              "      <td>ONLY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You fellas above says its the best Taiwanese r...</td>\n",
              "      <td>what do you think of the price ?</td>\n",
              "      <td>mediocre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BEST DINING EXPERIENCE IN THE WEST VILLAGE ! S...</td>\n",
              "      <td>do you like their decor ?</td>\n",
              "      <td>Highly impressed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you are the type of person who likes being ...</td>\n",
              "      <td>how many kinds of beers do they offer ?</td>\n",
              "      <td>over 100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you are the type of person who likes being ...</td>\n",
              "      <td>do you like their food ?</td>\n",
              "      <td>delicious</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Paragraph  ...            Answer\n",
              "0  You fellas above says its the best Taiwanese r...  ...              ONLY\n",
              "1  You fellas above says its the best Taiwanese r...  ...          mediocre\n",
              "2  BEST DINING EXPERIENCE IN THE WEST VILLAGE ! S...  ...  Highly impressed\n",
              "3  If you are the type of person who likes being ...  ...          over 100\n",
              "4  If you are the type of person who likes being ...  ...         delicious\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbL4P69ToSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_questions = []\n",
        "for ques in train_df['Question']:\n",
        "  tokenized_ques = bert_tokenizer.encode(ques, add_special_tokens=True, max_length = 6, pad_to_max_length=True, pad_token = '<pad>')\n",
        "  tokenized_questions.append(tokenized_ques)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yzFYtgwbLRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_questions = tokenized_questions[:576]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgc7CeQWVvjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(QUESTION_FILE, 'w') as q_file:\n",
        "  for tokeneized_ques in tokenized_questions:\n",
        "      ques = ' '.join([str(token) for token in tokeneized_ques])\n",
        "      q_file.write('%s\\n' % ques)\n",
        "q_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnvK1sM4Tobr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Genrator Parameters\n",
        "g_emb_dim = 768 # input_dim\n",
        "g_hidden_dim = 768\n",
        "g_sequence_len = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0C_L_dqVo3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_embedding(tokenized_sentences, length):\n",
        "  return bert_model(tokenized_sentences)[0].view(length, -1, g_emb_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7lumsKLXYaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(model, data_iter, criterion, optimizer):\n",
        "    total_loss = 0.\n",
        "    total_words = 0.\n",
        "    for (data, target) in data_iter:\n",
        "        data = Variable(data)\n",
        "        target = Variable(target)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        target = target.contiguous().view(-1)\n",
        "        pred = model.forward(data)\n",
        "        loss = criterion(pred, target)\n",
        "        total_loss += loss.item()\n",
        "        total_words += data.size(0) * data.size(1)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    data_iter.reset()\n",
        "    return math.exp(total_loss / total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afGYBsFSXYc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_epoch(model, data_iter, criterion):\n",
        "    total_loss = 0.\n",
        "    total_words = 0.\n",
        "    with torch.no_grad():\n",
        "        for (data, target) in data_iter:\n",
        "            data = Variable(data)\n",
        "            target = Variable(target)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.contiguous().view(-1)\n",
        "            pred = model.forward(data)\n",
        "            loss = criterion(pred, target)\n",
        "            total_loss += loss.item()\n",
        "            total_words += data.size(0) * data.size(1)\n",
        "        data_iter.reset()\n",
        "\n",
        "    assert total_words > 0  # Otherwise NullpointerException\n",
        "    return math.exp(total_loss / total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky6HQROuZv-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_samples(model, batch_size, generated_num, output_file):\n",
        "    samples = []\n",
        "    for _ in range(int(generated_num / batch_size)):\n",
        "        sample = model.sample(batch_size, g_sequence_len).cpu().data.numpy().tolist()\n",
        "        samples.extend(sample)\n",
        "    with open(output_file, 'w') as fout:\n",
        "        for sample in samples:\n",
        "            string = ' '.join([str(s) for s in sample])\n",
        "            fout.write('%s\\n' % string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q8ROqo0HQ42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GenDataIter(object):\n",
        "    \"\"\" Toy data iter to load digits\"\"\"\n",
        "    def __init__(self, data_file, batch_size):\n",
        "        super(GenDataIter, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_lis = self.read_file(data_file)\n",
        "        self.data_num = len(self.data_lis)\n",
        "        self.indices = range(self.data_num)\n",
        "        self.num_batches = int(math.ceil(float(self.data_num)/self.batch_size))\n",
        "        self.idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "\n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        random.shuffle(self.data_lis)\n",
        "\n",
        "    def next(self):\n",
        "        if self.idx >= self.data_num:\n",
        "            raise StopIteration\n",
        "        index = self.indices[self.idx:self.idx+self.batch_size]\n",
        "        d = [self.data_lis[i] for i in index]\n",
        "        d = torch.LongTensor(np.asarray(d, dtype='int64')).to(device)\n",
        "        #data = d\n",
        "        data = torch.cat([torch.zeros(self.batch_size, 1, device=device).long(), d], dim=1).to(device)\n",
        "        target = torch.cat([d, torch.zeros(self.batch_size, 1, device=device).long()], dim=1).to(device)\n",
        "        #target = torch.zeros(self.batch_size, 1).long()\n",
        "        self.idx += self.batch_size\n",
        "        return data, target\n",
        "\n",
        "    def read_file(self, data_file):\n",
        "        with open(data_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        lis = []\n",
        "        for line in lines:\n",
        "            l = line.strip().split(' ')\n",
        "            l = [int(s) for s in l]\n",
        "            lis.append(l)\n",
        "        return lis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8_lyWlok5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator \"\"\"\n",
        "    def __init__(self, num_emb, emb_dim, hidden_dim, device):\n",
        "        super(Generator, self).__init__()\n",
        "        self.num_emb = num_emb\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "        #self.emb = nn.Embedding(num_emb, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
        "        self.lin = nn.Linear(hidden_dim, num_emb)\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len), sequence of tokens generated by generator\n",
        "        \"\"\"\n",
        "        #emb = self.emb(x).to(self.device)\n",
        "        emb = bert_embedding(x, x.size(1))\n",
        "        h0, c0 = self.init_hidden(x.size(0))\n",
        "        h0, c0 = h0.to(self.device), c0.to(self.device)\n",
        "        self.lstm.flatten_parameters()\n",
        "        output, (h, c) = self.lstm(emb, (h0, c0))\n",
        "        output, (h, c) = output.to(self.device), (h.to(self.device), c.to(self.device))\n",
        "        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim))).to(self.device)\n",
        "        #pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim))).to(self.device)\n",
        "        return pred\n",
        "\n",
        "    def step(self, x, h, c):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size,  1), sequence of tokens generated by generator\n",
        "            h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            c: (1, batch_size, hidden_dim), lstm cell state\n",
        "        \"\"\"\n",
        "        #emb = self.emb(x).to(self.device)\n",
        "        emb = bert_embedding(x, x.size(1))\n",
        "        self.lstm.flatten_parameters()\n",
        "        output, (h, c) = self.lstm(emb, (h, c))\n",
        "        h, c = h.to(self.device), c.to(self.device)\n",
        "        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)), dim=1).to(self.device)\n",
        "        return pred, h, c\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim))).to(self.device)\n",
        "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim))).to(self.device)\n",
        "        return h, c\n",
        "\n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.uniform_(-0.05, 0.05)\n",
        "\n",
        "    def sample(self, batch_size, seq_len, x=None):\n",
        "        res = []\n",
        "        flag = False # whether sample from zero\n",
        "        if x is None:\n",
        "            flag = True\n",
        "        if flag:\n",
        "            x = Variable(torch.zeros((batch_size, 1)).long()).to(self.device)\n",
        "        h, c = self.init_hidden(batch_size)\n",
        "        samples = []\n",
        "        if flag:\n",
        "            for i in range(seq_len):\n",
        "                output, h, c = self.step(x, h, c)\n",
        "                x = output.multinomial(1)\n",
        "                samples.append(x)\n",
        "        else:\n",
        "            given_len = x.size(1)\n",
        "            lis = x.chunk(x.size(1), dim=1)\n",
        "            for i in range(given_len):\n",
        "                output, h, c = self.step(lis[i], h, c)\n",
        "                samples.append(lis[i])\n",
        "            x = output.multinomial(1)\n",
        "            for i in range(given_len, seq_len):\n",
        "                samples.append(x)\n",
        "                output, h, c = self.step(x, h, c)\n",
        "                x = output.multinomial(1)\n",
        "        output = torch.cat(samples, dim=1).to(self.device)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du25yJdPXYhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Networks\n",
        "generator = Generator(bert_tokenizer.vocab_size, g_emb_dim, g_hidden_dim, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEZeABneFLn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions_data_iter = GenDataIter(QUESTION_FILE, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQMxy6rJYAP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
        "gen_optimizer = optim.Adam(generator.parameters(), lr = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egsndw8HYAZG",
        "colab_type": "code",
        "outputId": "44e49299-d051-43ac-d7e2-49c6e93fdfc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "# Pretrain Generator using MLE #make the generator generate sentences similar to real sentences\n",
        "print('Pretrain with MLE ...')\n",
        "for epoch in range(PRE_EPOCH_NUM):\n",
        "    loss = train_epoch(generator, questions_data_iter, gen_criterion, gen_optimizer)\n",
        "    if epoch%10 == 0:\n",
        "      print('Epoch [%d] Model Loss: %f'% (epoch, loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretrain with MLE ...\n",
            "Epoch [0] Model Loss: 22838.567841\n",
            "Epoch [10] Model Loss: 9.060800\n",
            "Epoch [20] Model Loss: 4.020726\n",
            "Epoch [30] Model Loss: 2.814698\n",
            "Epoch [40] Model Loss: 2.319436\n",
            "Epoch [50] Model Loss: 2.003290\n",
            "Epoch [60] Model Loss: 1.787635\n",
            "Epoch [70] Model Loss: 1.652103\n",
            "Epoch [80] Model Loss: 1.541392\n",
            "Epoch [90] Model Loss: 1.456236\n",
            "Epoch [100] Model Loss: 1.382376\n",
            "Epoch [110] Model Loss: 1.326253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOualj2xch6B",
        "colab_type": "code",
        "outputId": "61bf7b13-5638-4d18-fc74-9896bde9e2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bert_tokenizer.convert_ids_to_tokens(generator.sample(1, 6).tolist()[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[unused584]', '[SEP]', '[PAD]', '[CLS]', 'reservation', 'how']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5FnlQU37BtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator Parameters\n",
        "d_emb_dim = 768 \n",
        "d_filter_sizes = [1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5]\n",
        "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLN9BoWN7BtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_dropout = 0.75\n",
        "d_num_class = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VzDu9V7Bth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GANLoss(nn.Module):\n",
        "    \"\"\"Reward-Refined NLLLoss Function for adversial training of Generator\"\"\"\n",
        "    def __init__(self):\n",
        "        super(GANLoss, self).__init__()\n",
        "\n",
        "    def forward(self, prob, target, reward):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            prob: (N, C), torch Variable\n",
        "            target : (N, ), torch Variable\n",
        "            reward : (N, ), torch Variable\n",
        "        \"\"\"\n",
        "        N = target.size(0)\n",
        "        C = prob.size(1)\n",
        "        one_hot = torch.zeros((N, C))\n",
        "        one_hot = one_hot.to(device)\n",
        "        one_hot.scatter_(1, target.data.view((-1,1)), 1)\n",
        "        one_hot = one_hot.type(torch.BoolTensor)\n",
        "        one_hot = Variable(one_hot)\n",
        "        one_hot = one_hot.to(device)\n",
        "        loss = torch.masked_select(prob, one_hot)\n",
        "        loss = loss * reward\n",
        "        loss =  -torch.sum(loss)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyWyc4D4hPa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"A CNN for text classification\n",
        "\n",
        "    architecture: Embedding >> Convolution >> Max-pooling >> Softmax\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, vocab_size, emb_dim, filter_sizes, num_filters, dropout):\n",
        "        super(Discriminator, self).__init__()\n",
        "        #self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, n, (f, emb_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
        "        ])\n",
        "        self.highway = nn.Linear(sum(num_filters), sum(num_filters))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lin = nn.Linear(sum(num_filters), num_classes)\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "        self.init_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size * seq_len)\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda:0' if torch.cuda. is_available() else 'cpu')\n",
        "        \n",
        "        #emb = self.emb(x).unsqueeze(1).to(device)  # batch_size * 1 * seq_len * emb_dim\n",
        "        emb = bert_embedding(x, x.size(1)).unsqueeze(1).view(BATCH_SIZE, 1, -1, d_emb_dim)\n",
        "        #print(emb.shape)\n",
        "        convs = [F.relu(conv(emb)).squeeze(3).to(device) for conv in self.convs]  # [batch_size * num_filter * length]\n",
        "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2).to(device) for conv in convs] # [batch_size * num_filter]\n",
        "        pred = torch.cat(pools, 1).to(device)  # batch_size * num_filters_sum\n",
        "        highway = self.highway(pred).to(device)\n",
        "        pred = torch.sigmoid(highway) *  F.relu(highway) + (1. - torch.sigmoid(highway)) * pred\n",
        "        pred = self.softmax(self.lin(self.dropout(pred))).to(device)\n",
        "        return pred\n",
        "\n",
        "    def init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.uniform_(-0.05, 0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irGsBIYzj12m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data_iter import DisDataIter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxUL9ZuB7Btj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator(d_num_class, bert_tokenizer.vocab_size, d_emb_dim, d_filter_sizes, d_num_filters, d_dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAEF5x4Z7Btw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dis_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
        "dis_optimizer = optim.Adam(discriminator.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ECjhhewBQl1",
        "colab_type": "code",
        "outputId": "99193e48-3538-4941-cef7-7594ea2bc4cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Pretrain Discriminator\n",
        "print('Pretrain Discriminator ...')\n",
        "for epoch in range(5):\n",
        "    generate_samples(generator, BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
        "    dis_data_iter = DisDataIter(QUESTION_FILE, NEGATIVE_FILE, BATCH_SIZE)\n",
        "    for _ in range(3):\n",
        "        loss = train_epoch(discriminator, dis_data_iter, dis_criterion, dis_optimizer)\n",
        "        print('Epoch [%d], loss: %f' % (epoch, loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretrain Discriminator ...\n",
            "Epoch [0], loss: 1.166704\n",
            "Epoch [0], loss: 1.045532\n",
            "Epoch [0], loss: 1.000227\n",
            "Epoch [1], loss: 1.000042\n",
            "Epoch [1], loss: 1.000668\n",
            "Epoch [1], loss: 1.000112\n",
            "Epoch [2], loss: 1.000088\n",
            "Epoch [2], loss: 1.000600\n",
            "Epoch [2], loss: 1.000264\n",
            "Epoch [3], loss: 1.000020\n",
            "Epoch [3], loss: 1.000077\n",
            "Epoch [3], loss: 1.000055\n",
            "Epoch [4], loss: 1.000260\n",
            "Epoch [4], loss: 1.001459\n",
            "Epoch [4], loss: 1.000147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJaL865cEmiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = generator.sample(64, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "031JRmfwFpwm",
        "colab_type": "code",
        "outputId": "e406bba7-f507-4040-8919-23eefa2072a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nn.Softmax(dim = 1)(discriminator(test_sentences)).argmax(dim = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVmVdtgMGIB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rollout(object):\n",
        "    \"\"\"Roll-out policy\"\"\"\n",
        "    def __init__(self, model, update_rate):\n",
        "        self.ori_model = model\n",
        "        self.own_model = copy.deepcopy(model)\n",
        "        self.update_rate = update_rate\n",
        "\n",
        "    def get_reward(self, x, num, discriminator):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x : (batch_size, seq_len) input data\n",
        "            num : roll-out number\n",
        "            discriminator : discrimanator model\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        for i in range(num):\n",
        "            for l in range(1, seq_len):\n",
        "                data = x[:, 0:l]\n",
        "                samples = self.own_model.sample(batch_size, seq_len, data)\n",
        "                pred = discriminator(samples)\n",
        "                pred = pred.cpu().data[:,1].numpy()\n",
        "                if i == 0:\n",
        "                    rewards.append(pred)\n",
        "                else:\n",
        "                    rewards[l-1] += pred\n",
        "\n",
        "            # for the last token\n",
        "            pred = discriminator(x)\n",
        "            pred = pred.cpu().data[:, 1].numpy()\n",
        "            if i == 0:\n",
        "                rewards.append(pred)\n",
        "            else:\n",
        "                rewards[seq_len-1] += pred\n",
        "        rewards = np.transpose(np.array(rewards)) / (1.0 * num) # batch_size * seq_len\n",
        "        return rewards\n",
        "\n",
        "    def update_params(self):\n",
        "        dic = {}\n",
        "        for name, param in self.ori_model.named_parameters():\n",
        "            dic[name] = param.data\n",
        "        for name, param in self.own_model.named_parameters():\n",
        "            if name.startswith('emb'):\n",
        "                param.data = dic[name]\n",
        "            else:\n",
        "                param.data = self.update_rate * param.data + (1 - self.update_rate) * dic[name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBVLaEEPBKuS",
        "colab_type": "code",
        "outputId": "687d79f9-62b9-4f1e-969e-4168066af2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Adversarial Training\n",
        "rollout = Rollout(generator, 0.8)\n",
        "print('Start Adeversatial Training...\\n')\n",
        "gen_gan_loss = GANLoss()\n",
        "gen_gan_optm = optim.Adam(generator.parameters())\n",
        "gen_gan_loss = gen_gan_loss.to(device)\n",
        "gen_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
        "dis_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
        "dis_optimizer = optim.Adam(discriminator.parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Adeversatial Training...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f-qtIpvg1gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = generator.sample(BATCH_SIZE, g_sequence_len)\n",
        "zeros = torch.zeros((BATCH_SIZE, 1)).type(torch.LongTensor).to(device)\n",
        "inputs = Variable(torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous())\n",
        "targets = Variable(samples.data).contiguous().view((-1,))\n",
        "rewards = rollout.get_reward(samples, 16, discriminator)\n",
        "rewards = Variable(torch.Tensor(rewards))\n",
        "rewards = torch.exp(rewards).contiguous().view((-1,))\n",
        "rewards = rewards.to(device)\n",
        "prob = generator.forward(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV1rJcT-X6YA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuuxSy64G9SA",
        "colab_type": "code",
        "outputId": "870f4707-e877-4f72-fcee-5181364dc5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "0dc4659ec9714836990d2a83f9a7f4cb",
            "999be904dbdb44e196af175037659b89",
            "704c09ecfd4f47d38c0fc7861ed1f8fb",
            "d1df2bce2ff54578b60015f96c7e91de",
            "4a3f3ce8887a44869eee7f7ae0638ea7",
            "32475c4a4be2414aa2929fe61147d189",
            "36db582cfc46484aa4112a64954d56c5",
            "903fd2501e11479cb6157d5a185b063e"
          ]
        }
      },
      "source": [
        "for total_batch in tqdm_notebook(range(TOTAL_BATCH)):\n",
        "    ## Train the generator for one step\n",
        "\n",
        "    for it in range(1):\n",
        "        samples = generator.sample(BATCH_SIZE, g_sequence_len)\n",
        "        # construct the input to the genrator, add zeros before samples and delete the last column\n",
        "        zeros = torch.zeros((BATCH_SIZE, 1)).type(torch.LongTensor).to(device)\n",
        "\n",
        "        inputs = Variable(torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous())\n",
        "        targets = Variable(samples.data).contiguous().view((-1,))\n",
        "        # calculate the reward\n",
        "        rewards = rollout.get_reward(samples, 16, discriminator)\n",
        "        rewards = Variable(torch.Tensor(rewards))\n",
        "        rewards = torch.exp(rewards).contiguous().view((-1,))\n",
        "        rewards = rewards.to(device)\n",
        "        prob = generator.forward(inputs)\n",
        "        loss = gen_gan_loss(prob, targets, rewards)\n",
        "        gen_gan_optm.zero_grad()\n",
        "        loss.backward()\n",
        "        gen_gan_optm.step()\n",
        "\n",
        "        if total_batch % 20 == 0:\n",
        "          print('Loss when TOTAL_BATCH = {} is {}'.format(total_batch, loss))\n",
        "\n",
        "    for _ in range(4):\n",
        "        generate_samples(generator, BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
        "        dis_data_iter = DisDataIter(QUESTION_FILE, NEGATIVE_FILE, BATCH_SIZE)\n",
        "        for _ in range(2):\n",
        "            loss = train_epoch(discriminator, dis_data_iter, dis_criterion, dis_optimizer)\n",
        "\n",
        "    if total_batch % 20 == 0:\n",
        "      print('Saving Generator and Discriminator')\n",
        "      torch.save(generator, 'generator_model.pt')\n",
        "      torch.save(discriminator, 'discriminator_model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dc4659ec9714836990d2a83f9a7f4cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss when TOTAL_BATCH = 0 is -0.0\n",
            "Saving Generator and Discriminator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss when TOTAL_BATCH = 20 is -0.0\n",
            "Saving Generator and Discriminator\n",
            "Loss when TOTAL_BATCH = 40 is -0.0\n",
            "Saving Generator and Discriminator\n",
            "Loss when TOTAL_BATCH = 60 is -0.0\n",
            "Saving Generator and Discriminator\n",
            "Loss when TOTAL_BATCH = 80 is -0.0\n",
            "Saving Generator and Discriminator\n",
            "Loss when TOTAL_BATCH = 100 is -0.0\n",
            "Saving Generator and Discriminator\n",
            "Loss when TOTAL_BATCH = 120 is -0.0\n",
            "Saving Generator and Discriminator\n",
            "Loss when TOTAL_BATCH = 140 is -0.0\n",
            "Saving Generator and Discriminator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOYZ_xl9Vdln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}