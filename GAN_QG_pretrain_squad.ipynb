{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fsxF5gI7Bs9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as po\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mlY1g3ORzCz"
   },
   "outputs": [],
   "source": [
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQ-m4BdB7BtG"
   },
   "outputs": [],
   "source": [
    "#from generator import Generator\n",
    "#from discriminator import Discriminator\n",
    "#from target_lstm import TargetLSTM\n",
    "#from rollout import Rollout\n",
    "#from data_iter  import GenDataIter, DisDataIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tszZwlH47BtJ"
   },
   "outputs": [],
   "source": [
    "# Basic Training Paramters\n",
    "SEED = 88\n",
    "BATCH_SIZE = 64\n",
    "TOTAL_BATCH = 200\n",
    "GENERATED_NUM = 10000\n",
    "POSITIVE_FILE = 'squad/real.data'\n",
    "NEGATIVE_FILE = 'squad/questions_generated.data'\n",
    "QUESTION_FILE = 'squad/questions_real.data'\n",
    "EVAL_FILE = 'squad/eval.data'\n",
    "PRE_EPOCH_NUM = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-JJBZaA7BtM"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKmftjzf7BtO"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda. is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDasC3sBBRkI"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('Bert'):\n",
    "  bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "  bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "  \n",
    "  os.mkdir('Bert')\n",
    "\n",
    "  bert_model.save_pretrained('Bert/')  \n",
    "  bert_tokenizer.save_pretrained('Bert/')\n",
    "\n",
    "else:\n",
    "  bert_model = BertModel.from_pretrained('Bert/')  \n",
    "  bert_tokenizer = BertTokenizer.from_pretrained('Bert/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EjREqPyNYFm2"
   },
   "outputs": [],
   "source": [
    "with open('data/squad/train-questions-only.pkl', 'rb') as file:\n",
    "  questions = pickle.load(file)\n",
    "questions = questions[:BATCH_SIZE*math.ceil(len(questions)/BATCH_SIZE)-BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "mG3JAHLGCl_F",
    "outputId": "582b22e6-372e-445e-84d8-44315538ff84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When did Beyonce start becoming popular?',\n",
       " 'What areas did Beyonce compete in when she was growing up?',\n",
       " \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       " 'In what city and state did Beyonce  grow up? ',\n",
       " 'In which decade did Beyonce become famous?',\n",
       " 'In what R&B group was she the lead singer?',\n",
       " 'What album made her a worldwide known artist?',\n",
       " \"Who managed the Destiny's Child group?\",\n",
       " 'When did Beyoncé rise to fame?',\n",
       " \"What role did Beyoncé have in Destiny's Child?\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "referenced_widgets": [
      "d5cc2e99599e405e9574c1c8a71d98c8",
      "d5e45cd77cd84948b9e94f2fe6078b01",
      "40702566c2ed477693f05d8845bea044",
      "618c6f25036342f6a0b074a18a687456",
      "eebae645984a4c68a9b79c6166bfce88",
      "4f027eb591d340889d2a7cb932dcf94b",
      "f9773bbfb5df48f5a735f8009330ef63",
      "263e1deee8e448dfa78186cb9ec694aa"
     ]
    },
    "colab_type": "code",
    "id": "1zJTlZdLW8v0",
    "outputId": "e8fe457c-4b80-4f8b-c255-7141cc92c8ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cc2e99599e405e9574c1c8a71d98c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=130304), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_questions = []\n",
    "for ques in tqdm_notebook(questions):\n",
    "  tokenized_ques = bert_tokenizer.encode(ques, add_special_tokens=True)\n",
    "  tokenized_questions.append(tokenized_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "f8OW8Bf7Ynf9",
    "outputId": "d9974d8f-5e62-41b2-c2f2-0fe73153c292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_lens = [len(tok_ques) for tok_ques in tokenized_questions]\n",
    "max(ques_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vL5ZEdaalyGQ"
   },
   "outputs": [],
   "source": [
    "del tokenized_questions[np.argmax(np.array(ques_lens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XDt1_Y4LmKrE",
    "outputId": "e8d451a7-92a6-4222-a918-f3644a15f827"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_lens = [len(tok_ques) for tok_ques in tokenized_questions]\n",
    "max(ques_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGObAlCfmNq1"
   },
   "outputs": [],
   "source": [
    "del tokenized_questions[np.argmax(np.array(ques_lens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QeMefoVMmNtD",
    "outputId": "49ed34a6-b78a-4708-b177-c0ba65252a28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_lens = [len(tok_ques) for tok_ques in tokenized_questions]\n",
    "max(ques_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kU2JfPuhmNxz"
   },
   "outputs": [],
   "source": [
    "del tokenized_questions[np.argmax(np.array(ques_lens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KS_xeSjemNvS",
    "outputId": "b4a4cad7-af8a-4b7b-c16a-b6959479e957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_lens = [len(tok_ques) for tok_ques in tokenized_questions]\n",
    "max(ques_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "referenced_widgets": [
      "af32c645e80d43a8a05cbbbf542fa8fe",
      "e45ff8e57e7f4a16b4ff4d9651060867",
      "6c7930efb62b4f46a7231664adf9a3d4",
      "ad23a55e90ac4aaaa9eb42e7ae19b64d",
      "24eef061811c4e08a9c8afa11200b8b1",
      "3f16ddf3a42343d092454e675a77a46d",
      "9da8789422c2412ea9fde0527bd302d6",
      "9e74cb7238ae4ffea742c38c57fc39e0"
     ]
    },
    "colab_type": "code",
    "id": "eT-9TOdVYuvQ",
    "outputId": "ed20851b-4ac2-4e7f-82d3-a40a873f4c31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af32c645e80d43a8a05cbbbf542fa8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=130304), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_questions = []\n",
    "for ques in tqdm_notebook(questions):\n",
    "  tokenized_ques = bert_tokenizer.encode(ques, add_special_tokens=True, max_length = max(ques_lens), pad_to_max_length=True)\n",
    "  tokenized_questions.append(tokenized_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1-iE5EyYzix"
   },
   "outputs": [],
   "source": [
    "with open(QUESTION_FILE, 'w') as q_file:\n",
    "  for tokeneized_ques in tokenized_questions:\n",
    "      ques = ' '.join([str(token) for token in tokeneized_ques])\n",
    "      q_file.write('%s\\n' % ques)\n",
    "q_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTkegP5rY0gT"
   },
   "outputs": [],
   "source": [
    "tokenized_questions_t = torch.tensor(tokenized_questions, device = 'cpu')\n",
    "tokenized_questions_t = tokenized_questions_t.view(tokenized_questions_t.size(1), tokenized_questions_t.size(0))\n",
    "ques_lens_t = torch.tensor(ques_lens, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jlorkL7XY3uS",
    "outputId": "b52209be-7b59-4f31-84b6-d59b377d00a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1841439])"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.nn.utils.rnn.pack_padded_sequence(tokenized_questions_t, ques_lens_t, enforce_sorted=False)\n",
    "X.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cixTf9U4oFGq",
    "outputId": "278f9ad6-ab79-40ba-9125-c21150355eff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfLHIYZ0Chdt"
   },
   "outputs": [],
   "source": [
    "# Generator Parameters\n",
    "g_emb_dim = 768 # input_dim\n",
    "g_hidden_dim = 768\n",
    "g_sequence_len = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5JRepHyCkRq"
   },
   "outputs": [],
   "source": [
    "def bert_embedding(tokenized_sentences, length):\n",
    "  return bert_model(tokenized_sentences)[0].view(length, -1, g_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ-YDnhW5_tA"
   },
   "outputs": [],
   "source": [
    "class GenDataIter(object):\n",
    "    \"\"\" Toy data iter to load digits\"\"\"\n",
    "    def __init__(self, data_file, batch_size):\n",
    "        super(GenDataIter, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_lis, self.data_lens = self.read_file(data_file)\n",
    "        self.data_num = len(self.data_lis)\n",
    "        self.indices = range(self.data_num)\n",
    "        self.num_batches = int(math.ceil(float(self.data_num)/self.batch_size))\n",
    "        self.idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.idx = 0\n",
    "        random.shuffle(self.data_lis)\n",
    "\n",
    "    def next(self):\n",
    "        if self.idx >= self.data_num:\n",
    "            raise StopIteration\n",
    "        index = self.indices[self.idx:self.idx+self.batch_size]\n",
    "        pad_ques = [self.data_lis[i] for i in index]\n",
    "        lens = [self.data_lens[i] for i in index]\n",
    "        pad_ques_t = torch.tensor(pad_ques).transpose(0, 1)\n",
    "        lens_t = torch.LongTensor(lens, device = 'cpu')\n",
    "        target = torch.zeros(self.batch_size, 1).long()\n",
    "        self.idx += self.batch_size\n",
    "        return (pad_ques_t, lens_t), target\n",
    "\n",
    "    def read_file(self, data_file):\n",
    "\n",
    "        with open(data_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        lens = []\n",
    "        lis = []\n",
    "        for line in lines:\n",
    "            l = line.strip().split(' ')\n",
    "            l = [int(s) for s in l]\n",
    "            lis.append(l)\n",
    "\n",
    "            len_l = 0\n",
    "            for i, id in enumerate(l):\n",
    "              if id != 0:\n",
    "                len_l = i + 1\n",
    "            lens.append(len_l)\n",
    "\n",
    "        return lis, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDRUrJctFL08"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator \"\"\"\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, device):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        #self.emb = nn.Embedding(num_emb, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, 2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len), sequence of tokens generated by generator\n",
    "        \"\"\"\n",
    "        #emb = self.emb(x).to(self.device)\n",
    "        \n",
    "        pad_ques, lens = data[0], data[1]\n",
    "        emb = bert_embedding(pad_ques, pad_ques.size(0)).to(self.device)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(emb, lens, enforce_sorted=False).to(self.device)\n",
    "        h0, c0 = self.init_hidden(pad_ques.size(1))\n",
    "        h0, c0 = h0.to(self.device), c0.to(self.device)\n",
    "        self.lstm.flatten_parameters()\n",
    "        packed_output, (h, c) = self.lstm(x, (h0, c0))\n",
    "        packed_output, (h, c) = packed_output.to(self.device), (h.to(self.device), c.to(self.device))\n",
    "        pred = self.softmax(self.lin(h.contiguous().view(-1, g_hidden_dim))).to(self.device)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def step(self, x, h, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size,  1), sequence of tokens generated by generator\n",
    "            h: (1, batch_size, hidden_dim), lstm hidden state\n",
    "            c: (1, batch_size, hidden_dim), lstm cell state\n",
    "        \"\"\"\n",
    "        #emb = self.emb(x).to(self.device)\n",
    "        emb = bert_embedding(x, x.size(1))\n",
    "        self.lstm.flatten_parameters()\n",
    "        output, (h, c) = self.lstm(emb, (h, c))\n",
    "        h, c = h.to(self.device), c.to(self.device)\n",
    "        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)), dim=1).to(self.device)\n",
    "        return pred, h, c\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim))).to(self.device)\n",
    "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim))).to(self.device)\n",
    "        return h, c\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "    def sample(self, batch_size, seq_len, x=None):\n",
    "        res = []\n",
    "        flag = False # whether sample from zero\n",
    "        if x is None:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            x = Variable(torch.zeros((batch_size, 1)).long()).to(self.device)\n",
    "        h, c = self.init_hidden(batch_size)\n",
    "        samples = []\n",
    "        if flag:\n",
    "            for i in range(seq_len):\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                x = output.multinomial(1)\n",
    "                samples.append(x)\n",
    "        else:\n",
    "            given_len = x.size(1)\n",
    "            lis = x.chunk(x.size(1), dim=1)\n",
    "            for i in range(given_len):\n",
    "                output, h, c = self.step(lis[i], h, c)\n",
    "                samples.append(lis[i])\n",
    "            x = output.multinomial(1)\n",
    "            for i in range(given_len, seq_len):\n",
    "                samples.append(x)\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                x = output.multinomial(1)\n",
    "        output = torch.cat(samples, dim=1).to(self.device)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IofcJxy6Lsb"
   },
   "outputs": [],
   "source": [
    "questions_data_iter = GenDataIter(QUESTION_FILE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nruLKSRiFPgu"
   },
   "outputs": [],
   "source": [
    "generator = Generator(bert_tokenizer.vocab_size, g_emb_dim, g_hidden_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_iter, criterion, optimizer):\n",
    "    total_loss = 0.\n",
    "    total_words = 0.\n",
    "    for (data, target) in data_iter:\n",
    "        #data = Variable(data)\n",
    "        target = Variable(target).to(device)\n",
    "        target = target.contiguous().view(-1)\n",
    "        pred = model.forward(data)\n",
    "        loss = criterion(pred, target)\n",
    "        total_loss += loss.item()\n",
    "        total_words += data[0].size(0) * data[0].size(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    data_iter.reset()\n",
    "    return math.exp(total_loss / total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, data_iter, criterion):\n",
    "    total_loss = 0.\n",
    "    total_words = 0.\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in data_iter:\n",
    "            #data = Variable(data)\n",
    "            target = Variable(target).to(device)\n",
    "            target = target.contiguous().view(-1)\n",
    "            pred = model.forward(data)\n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item()\n",
    "            total_words += data.size(0) * data.size(1)\n",
    "        data_iter.reset()\n",
    "\n",
    "    assert total_words > 0  # Otherwise NullpointerException\n",
    "    return math.exp(total_loss / total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_criterion = nn.CrossEntropyLoss().to(device)\n",
    "gen_optimizer = optim.Adam(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain with MLE ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d1ec01083f4eca9ef488c400ab52f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=120), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8f28d3ce29ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pretrain with MLE ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRE_EPOCH_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch [%d] Model Loss: %f'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-fd47f1f30445>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_iter, criterion, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdata_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pretrain Generator using MLE #make the generator generate sentences similar to real sentences\n",
    "print('Pretrain with MLE ...')\n",
    "for epoch in tqdm_notebook(range(PRE_EPOCH_NUM)):\n",
    "    loss = train_epoch(generator, questions_data_iter, gen_criterion, gen_optimizer)\n",
    "    if epoch%10 == 0:\n",
    "      print('Epoch [%d] Model Loss: %f'% (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "d1imxakn6V_7",
    "outputId": "04b518f2-5950-4fe1-dc28-c82a5941c692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "for data, target in questions_data_iter:\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    print(target.shape)\n",
    "    #targets = torch.zeros([BATCH_SIZE, 1])\n",
    "    pred = generator.forward(data)\n",
    "    print(pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afGYBsFSXYc1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ky6HQROuZv-u"
   },
   "outputs": [],
   "source": [
    "def generate_samples(model, batch_size, generated_num, output_file):\n",
    "    samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        sample = model.sample(batch_size, g_sequence_len).cpu().data.numpy().tolist()\n",
    "        samples.extend(sample)\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for sample in samples:\n",
    "            string = ' '.join([str(s) for s in sample])\n",
    "            fout.write('%s\\n' % string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "du25yJdPXYhB"
   },
   "outputs": [],
   "source": [
    "# Define Networks\n",
    "generator = Generator(bert_tokenizer.vocab_size, g_emb_dim, g_hidden_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "bEZeABneFLn5",
    "outputId": "899272f4-e4cf-45a0-ac8d-ba21e68fbc18"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-85718b1c3577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquestions_data_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenDataIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQUESTION_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'GenDataIter' is not defined"
     ]
    }
   ],
   "source": [
    "questions_data_iter = GenDataIter(QUESTION_FILE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQMxy6rJYAP0"
   },
   "outputs": [],
   "source": [
    "gen_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "egsndw8HYAZG",
    "outputId": "44e49299-d051-43ac-d7e2-49c6e93fdfc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain with MLE ...\n",
      "Epoch [0] Model Loss: 22838.567841\n",
      "Epoch [10] Model Loss: 9.060800\n",
      "Epoch [20] Model Loss: 4.020726\n",
      "Epoch [30] Model Loss: 2.814698\n",
      "Epoch [40] Model Loss: 2.319436\n",
      "Epoch [50] Model Loss: 2.003290\n",
      "Epoch [60] Model Loss: 1.787635\n",
      "Epoch [70] Model Loss: 1.652103\n",
      "Epoch [80] Model Loss: 1.541392\n",
      "Epoch [90] Model Loss: 1.456236\n",
      "Epoch [100] Model Loss: 1.382376\n",
      "Epoch [110] Model Loss: 1.326253\n"
     ]
    }
   ],
   "source": [
    "# Pretrain Generator using MLE #make the generator generate sentences similar to real sentences\n",
    "print('Pretrain with MLE ...')\n",
    "for epoch in range(PRE_EPOCH_NUM):\n",
    "    loss = train_epoch(generator, questions_data_iter, gen_criterion, gen_optimizer)\n",
    "    if epoch%10 == 0:\n",
    "      print('Epoch [%d] Model Loss: %f'% (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hOualj2xch6B",
    "outputId": "61bf7b13-5638-4d18-fc74-9896bde9e2ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[unused584]', '[SEP]', '[PAD]', '[CLS]', 'reservation', 'how']"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_ids_to_tokens(generator.sample(1, 6).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5FnlQU37BtU"
   },
   "outputs": [],
   "source": [
    "# Discriminator Parameters\n",
    "d_emb_dim = 768 \n",
    "d_filter_sizes = [1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5]\n",
    "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLN9BoWN7BtW"
   },
   "outputs": [],
   "source": [
    "d_dropout = 0.75\n",
    "d_num_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_VzDu9V7Bth"
   },
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Reward-Refined NLLLoss Function for adversial training of Generator\"\"\"\n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prob, target, reward):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prob: (N, C), torch Variable\n",
    "            target : (N, ), torch Variable\n",
    "            reward : (N, ), torch Variable\n",
    "        \"\"\"\n",
    "        N = target.size(0)\n",
    "        C = prob.size(1)\n",
    "        one_hot = torch.zeros((N, C))\n",
    "        one_hot = one_hot.to(device)\n",
    "        one_hot.scatter_(1, target.data.view((-1,1)), 1)\n",
    "        one_hot = one_hot.type(torch.BoolTensor)\n",
    "        one_hot = Variable(one_hot)\n",
    "        one_hot = one_hot.to(device)\n",
    "        loss = torch.masked_select(prob, one_hot)\n",
    "        loss = loss * reward\n",
    "        loss =  -torch.sum(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VyWyc4D4hPa7"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"A CNN for text classification\n",
    "\n",
    "    architecture: Embedding >> Convolution >> Max-pooling >> Softmax\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, vocab_size, emb_dim, filter_sizes, num_filters, dropout):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, emb_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(sum(num_filters), sum(num_filters))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lin = nn.Linear(sum(num_filters), num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.init_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size * seq_len)\n",
    "        \"\"\"\n",
    "        device = torch.device('cuda:0' if torch.cuda. is_available() else 'cpu')\n",
    "        \n",
    "        #emb = self.emb(x).unsqueeze(1).to(device)  # batch_size * 1 * seq_len * emb_dim\n",
    "        emb = bert_embedding(x, x.size(1)).unsqueeze(1).view(BATCH_SIZE, 1, -1, d_emb_dim)\n",
    "        #print(emb.shape)\n",
    "        convs = [F.relu(conv(emb)).squeeze(3).to(device) for conv in self.convs]  # [batch_size * num_filter * length]\n",
    "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2).to(device) for conv in convs] # [batch_size * num_filter]\n",
    "        pred = torch.cat(pools, 1).to(device)  # batch_size * num_filters_sum\n",
    "        highway = self.highway(pred).to(device)\n",
    "        pred = torch.sigmoid(highway) *  F.relu(highway) + (1. - torch.sigmoid(highway)) * pred\n",
    "        pred = self.softmax(self.lin(self.dropout(pred))).to(device)\n",
    "        return pred\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irGsBIYzj12m"
   },
   "outputs": [],
   "source": [
    "from data_iter import DisDataIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxUL9ZuB7Btj"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator(d_num_class, bert_tokenizer.vocab_size, d_emb_dim, d_filter_sizes, d_num_filters, d_dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAEF5x4Z7Btw"
   },
   "outputs": [],
   "source": [
    "dis_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "0ECjhhewBQl1",
    "outputId": "99193e48-3538-4941-cef7-7594ea2bc4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Discriminator ...\n",
      "Epoch [0], loss: 1.166704\n",
      "Epoch [0], loss: 1.045532\n",
      "Epoch [0], loss: 1.000227\n",
      "Epoch [1], loss: 1.000042\n",
      "Epoch [1], loss: 1.000668\n",
      "Epoch [1], loss: 1.000112\n",
      "Epoch [2], loss: 1.000088\n",
      "Epoch [2], loss: 1.000600\n",
      "Epoch [2], loss: 1.000264\n",
      "Epoch [3], loss: 1.000020\n",
      "Epoch [3], loss: 1.000077\n",
      "Epoch [3], loss: 1.000055\n",
      "Epoch [4], loss: 1.000260\n",
      "Epoch [4], loss: 1.001459\n",
      "Epoch [4], loss: 1.000147\n"
     ]
    }
   ],
   "source": [
    "# Pretrain Discriminator\n",
    "print('Pretrain Discriminator ...')\n",
    "for epoch in range(5):\n",
    "    generate_samples(generator, BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "    dis_data_iter = DisDataIter(QUESTION_FILE, NEGATIVE_FILE, BATCH_SIZE)\n",
    "    for _ in range(3):\n",
    "        loss = train_epoch(discriminator, dis_data_iter, dis_criterion, dis_optimizer)\n",
    "        print('Epoch [%d], loss: %f' % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJaL865cEmiM"
   },
   "outputs": [],
   "source": [
    "test_sentences = generator.sample(64, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "031JRmfwFpwm",
    "outputId": "e406bba7-f507-4040-8919-23eefa2072a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim = 1)(discriminator(test_sentences)).argmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVmVdtgMGIB2"
   },
   "outputs": [],
   "source": [
    "class Rollout(object):\n",
    "    \"\"\"Roll-out policy\"\"\"\n",
    "    def __init__(self, model, update_rate):\n",
    "        self.ori_model = model\n",
    "        self.own_model = copy.deepcopy(model)\n",
    "        self.update_rate = update_rate\n",
    "\n",
    "    def get_reward(self, x, num, discriminator):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : (batch_size, seq_len) input data\n",
    "            num : roll-out number\n",
    "            discriminator : discrimanator model\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        for i in range(num):\n",
    "            for l in range(1, seq_len):\n",
    "                data = x[:, 0:l]\n",
    "                samples = self.own_model.sample(batch_size, seq_len, data)\n",
    "                pred = discriminator(samples)\n",
    "                pred = pred.cpu().data[:,1].numpy()\n",
    "                if i == 0:\n",
    "                    rewards.append(pred)\n",
    "                else:\n",
    "                    rewards[l-1] += pred\n",
    "\n",
    "            # for the last token\n",
    "            pred = discriminator(x)\n",
    "            pred = pred.cpu().data[:, 1].numpy()\n",
    "            if i == 0:\n",
    "                rewards.append(pred)\n",
    "            else:\n",
    "                rewards[seq_len-1] += pred\n",
    "        rewards = np.transpose(np.array(rewards)) / (1.0 * num) # batch_size * seq_len\n",
    "        return rewards\n",
    "\n",
    "    def update_params(self):\n",
    "        dic = {}\n",
    "        for name, param in self.ori_model.named_parameters():\n",
    "            dic[name] = param.data\n",
    "        for name, param in self.own_model.named_parameters():\n",
    "            if name.startswith('emb'):\n",
    "                param.data = dic[name]\n",
    "            else:\n",
    "                param.data = self.update_rate * param.data + (1 - self.update_rate) * dic[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "NBVLaEEPBKuS",
    "outputId": "687d79f9-62b9-4f1e-969e-4168066af2b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Adeversatial Training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adversarial Training\n",
    "rollout = Rollout(generator, 0.8)\n",
    "print('Start Adeversatial Training...\\n')\n",
    "gen_gan_loss = GANLoss()\n",
    "gen_gan_optm = optim.Adam(generator.parameters())\n",
    "gen_gan_loss = gen_gan_loss.to(device)\n",
    "gen_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
    "dis_criterion = nn.NLLLoss(reduction='sum').to(device)\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9f-qtIpvg1gh"
   },
   "outputs": [],
   "source": [
    "samples = generator.sample(BATCH_SIZE, g_sequence_len)\n",
    "zeros = torch.zeros((BATCH_SIZE, 1)).type(torch.LongTensor).to(device)\n",
    "inputs = Variable(torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous())\n",
    "targets = Variable(samples.data).contiguous().view((-1,))\n",
    "rewards = rollout.get_reward(samples, 16, discriminator)\n",
    "rewards = Variable(torch.Tensor(rewards))\n",
    "rewards = torch.exp(rewards).contiguous().view((-1,))\n",
    "rewards = rewards.to(device)\n",
    "prob = generator.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iV1rJcT-X6YA"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "0dc4659ec9714836990d2a83f9a7f4cb",
      "999be904dbdb44e196af175037659b89",
      "704c09ecfd4f47d38c0fc7861ed1f8fb",
      "d1df2bce2ff54578b60015f96c7e91de",
      "4a3f3ce8887a44869eee7f7ae0638ea7",
      "32475c4a4be2414aa2929fe61147d189",
      "36db582cfc46484aa4112a64954d56c5",
      "903fd2501e11479cb6157d5a185b063e"
     ]
    },
    "colab_type": "code",
    "id": "XuuxSy64G9SA",
    "outputId": "5d9a214f-ff54-4990-e4d5-2074fcf44ef8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc4659ec9714836990d2a83f9a7f4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss when TOTAL_BATCH = 0 is -0.0\n",
      "Saving Generator and Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss when TOTAL_BATCH = 20 is -0.0\n"
     ]
    }
   ],
   "source": [
    "for total_batch in tqdm_notebook(range(TOTAL_BATCH)):\n",
    "    ## Train the generator for one step\n",
    "\n",
    "    for it in range(1):\n",
    "        samples = generator.sample(BATCH_SIZE, g_sequence_len)\n",
    "        # construct the input to the genrator, add zeros before samples and delete the last column\n",
    "        zeros = torch.zeros((BATCH_SIZE, 1)).type(torch.LongTensor).to(device)\n",
    "\n",
    "        inputs = Variable(torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous())\n",
    "        targets = Variable(samples.data).contiguous().view((-1,))\n",
    "        # calculate the reward\n",
    "        rewards = rollout.get_reward(samples, 16, discriminator)\n",
    "        rewards = Variable(torch.Tensor(rewards))\n",
    "        rewards = torch.exp(rewards).contiguous().view((-1,))\n",
    "        rewards = rewards.to(device)\n",
    "        prob = generator.forward(inputs)\n",
    "        loss = gen_gan_loss(prob, targets, rewards)\n",
    "        gen_gan_optm.zero_grad()\n",
    "        loss.backward()\n",
    "        gen_gan_optm.step()\n",
    "\n",
    "        if total_batch % 20 == 0:\n",
    "          print('Loss when TOTAL_BATCH = {} is {}'.format(total_batch, loss))\n",
    "\n",
    "    for _ in range(4):\n",
    "        generate_samples(generator, BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "        dis_data_iter = DisDataIter(QUESTION_FILE, NEGATIVE_FILE, BATCH_SIZE)\n",
    "        for _ in range(2):\n",
    "            loss = train_epoch(discriminator, dis_data_iter, dis_criterion, dis_optimizer)\n",
    "\n",
    "    if total_batch % 20 == 0:\n",
    "      print('Saving Generator and Discriminator')\n",
    "      torch.save(generator, 'generator_model.pt')\n",
    "      torch.save(discriminator, 'discriminator_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOYZ_xl9Vdln"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GAN_QG_pretrain_squad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0dc4659ec9714836990d2a83f9a7f4cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_704c09ecfd4f47d38c0fc7861ed1f8fb",
       "IPY_MODEL_d1df2bce2ff54578b60015f96c7e91de"
      ],
      "layout": "IPY_MODEL_999be904dbdb44e196af175037659b89"
     }
    },
    "24eef061811c4e08a9c8afa11200b8b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "263e1deee8e448dfa78186cb9ec694aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32475c4a4be2414aa2929fe61147d189": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36db582cfc46484aa4112a64954d56c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f16ddf3a42343d092454e675a77a46d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40702566c2ed477693f05d8845bea044": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f027eb591d340889d2a7cb932dcf94b",
      "max": 130304,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eebae645984a4c68a9b79c6166bfce88",
      "value": 130304
     }
    },
    "4a3f3ce8887a44869eee7f7ae0638ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f027eb591d340889d2a7cb932dcf94b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "618c6f25036342f6a0b074a18a687456": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_263e1deee8e448dfa78186cb9ec694aa",
      "placeholder": "​",
      "style": "IPY_MODEL_f9773bbfb5df48f5a735f8009330ef63",
      "value": " 130304/130304 [06:47&lt;00:00, 319.92it/s]"
     }
    },
    "6c7930efb62b4f46a7231664adf9a3d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f16ddf3a42343d092454e675a77a46d",
      "max": 130304,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24eef061811c4e08a9c8afa11200b8b1",
      "value": 130304
     }
    },
    "704c09ecfd4f47d38c0fc7861ed1f8fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32475c4a4be2414aa2929fe61147d189",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a3f3ce8887a44869eee7f7ae0638ea7",
      "value": 20
     }
    },
    "903fd2501e11479cb6157d5a185b063e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "999be904dbdb44e196af175037659b89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da8789422c2412ea9fde0527bd302d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e74cb7238ae4ffea742c38c57fc39e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad23a55e90ac4aaaa9eb42e7ae19b64d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e74cb7238ae4ffea742c38c57fc39e0",
      "placeholder": "​",
      "style": "IPY_MODEL_9da8789422c2412ea9fde0527bd302d6",
      "value": " 130304/130304 [07:42&lt;00:00, 282.04it/s]"
     }
    },
    "af32c645e80d43a8a05cbbbf542fa8fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6c7930efb62b4f46a7231664adf9a3d4",
       "IPY_MODEL_ad23a55e90ac4aaaa9eb42e7ae19b64d"
      ],
      "layout": "IPY_MODEL_e45ff8e57e7f4a16b4ff4d9651060867"
     }
    },
    "d1df2bce2ff54578b60015f96c7e91de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_903fd2501e11479cb6157d5a185b063e",
      "placeholder": "​",
      "style": "IPY_MODEL_36db582cfc46484aa4112a64954d56c5",
      "value": " 10% 20/200 [42:59&lt;6:26:35, 128.87s/it]"
     }
    },
    "d5cc2e99599e405e9574c1c8a71d98c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40702566c2ed477693f05d8845bea044",
       "IPY_MODEL_618c6f25036342f6a0b074a18a687456"
      ],
      "layout": "IPY_MODEL_d5e45cd77cd84948b9e94f2fe6078b01"
     }
    },
    "d5e45cd77cd84948b9e94f2fe6078b01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e45ff8e57e7f4a16b4ff4d9651060867": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eebae645984a4c68a9b79c6166bfce88": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f9773bbfb5df48f5a735f8009330ef63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
